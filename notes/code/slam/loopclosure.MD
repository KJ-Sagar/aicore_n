# Loop Closure

Loop closure is the process of identifying and correcting accumulated pose estimation errors when a robot revisits a previously visited location. It involves recognizing that the current sensor measurements align with a previously recorded portion of the map, thus closing a loop in the trajectory. Loop closure detection is indispensable for long-term and large-scale SLAM operations, as it helps mitigate drift and ensures a globally consistent map.

Goal :

    + Understand why loop closure is needed in SLAM.
    + Understand the principles of the bag-of-words model.
    + Use DBoW3 to detect similar images.

We know that the primary purpose of SLAM (frontend, backend) is to estimate camera movement. However, the loop closure module is quite different.

### Loop Closure and Detection :

Problem Statement: The frontend provides short-time trajectory/landmarks estimation and the map’s initial value. The backend is responsible for optimizing all these data. However, if we only consider the adjacent keyframes like VO, then the errors will inevitably accumulate with time so that the entire SLAM will suffer from the accumulative error. The result of long-term estimation will not be reliable. In other words, we cannot construct globally consistent trajectories and maps.

##  Bag of Words (BoW) :

The purpose of Bag-of-Words (BoW) is to describe an image in terms of “what
kinds of features are there on the image.” For example, we say a person and a car in
one photo; and two people and a dog in another photo. According to this description,
the similarity of the two images can be measured. To be more specific, we need to
do the following things:
1. Determine the concepts of people, cars, and dogs-corresponding to the word.
Many words are put together to form a dictionary.
2. Detect which predefined words in the dictionary appear in an image-we use the
appearance of words (histogram) to describe the entire image. In this way, we
convert an image into a vector description.
3. Compute the similarity by the histogram in the second step.

[ Train the dictionary, Calculate the similarity ]

```cpp

#include "DBoW3/DBoW3.h"
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <iostream>
#include <vector>
#include <string>

using namespace cv;
using namespace std;

/**************************************************** **
  * This section demonstrates how to calculate the similarity score based on the previously trained dictionary
  * ***************************************************/
int main(int argc, char **argv) {
     // read the images and database
     cout << "reading database" << endl;
     DBoW3::Vocabulary vocab("./vocabulary.yml.gz");
     // DBoW3::Vocabulary vocab("./vocab_larger.yml.gz"); // use large vocab if you want:
     if (vocab. empty()) {
         cerr << "Vocabulary does not exist." << endl;
         return 1;
     }
     cout << "reading images... " << endl;
     vector<Mat> images;
     for (int i = 0; i < 10; i++) {
         string path = "./data/" + to_string(i + 1) + ".png";
         images.push_back(imread(path));
     }

     // NOTE: in this case we are comparing images with a vocabulary generated by themselves,
     //  this may lead to overfit.
     // detect ORB features
     cout << "detecting ORB features ... " << endl;
     Ptr<Feature2D> detector = ORB::create();
     vector<Mat> descriptors;
     for (Mat &image:images) {
         vector<KeyPoint> keypoints;
         Mat descriptor;
         detector->detectAndCompute(image, Mat(), keypoints, descriptor);
         descriptors.push_back(descriptor);
     }

     // we can compare the images directly or we can compare one image to a database
     // images:
     cout << "comparing images with images " << endl;
     for (int i = 0; i < images. size(); i++) {
         DBoW3::BowVector v1;
         vocab. transform(descriptors[i], v1);
         for (int j = i; j < images. size(); j++) {
             DBoW3::BowVector v2;
             vocab. transform(descriptors[j], v2);
             double score = vocab. score(v1, v2);
             cout << "image " << i << " vs image " << j << " : " << score << endl;
         }
         cout << endl;
     }

     // or with database
     cout << "comparing images with database " << endl;
     DBoW3::Database db(vocab, false, 0);
     for (int i = 0; i < descriptors. size(); i++)
         db.add(descriptors[i]);
     cout << "database info: " << db << endl;
     for (int i = 0; i < descriptors. size(); i++) {
         DBoW3::QueryResults ret;
         db. query(descriptors[i], ret, 4); // max result=4
         cout << "searching for image " << i << " returns " << ret << endl << endl;
     }
     cout << "done." << endl;
}

```

[ Increasing the Dictionary Scale, Similarity Score Processing, Processing the Keyframes, Validation of the Detected Loops, ML in Loop Closing ]

## Role of Deep Learning in Loop Closure Detection for Visual and Lidar SLAM:

Loop closure detection is of vital importance in the process of simultaneous localization and mapping (SLAM), as it helps to reduce the cumulative error of the robot’s estimated pose and generate a consistent global map.

 Summary of the benefits and limitations of camera and Lidar-based loop closure detection methods:

 <table width=100%>
 <tr>
 <th></th>
 <th>Method</th>
 <th></th>
 <th>Benefits</th>
 <th>Limitations</th>
 </tr>
 <tr>
 <td></td>
 <td>Image-to-Image</td>
 <td>Offline
Vocabulary</td>
 <td>

+ Does not require the metric information of the features
+ Dependent on the appearance features and their existence in dictionary
+ Good for loop detection with planar camera motion
 </td>
 <td>
 
+ Not suitable for dynamic robot environment
+ Memory consumption is proportional to vocabulary size
+ Performance reduces if tested on different dataset.
 </td>
 </tr>
 

 
 <tr>
 <td>Vision-based</td>
 <td>Image-to-Image</td>
 <td>Online Vocabulary	
</td>
 <td>

+ Allows to learn features in real time
 </td>
 <td>
 
+ Memory consumption is proportional to vocabulary size
+ Does not use geometric information
 </td>
 </tr>


  <tr>
 <td></td>
 <td>Map-to-Map</td>
 <td></td>
 <td>

+ Detects true loops when common features exist in two submaps
 </td>
 <td>
 
+ Not suitable for sparse maps.
+ Cannot achieve high performance for complex dense maps.
 </td>
 </tr>


   <tr>
 <td></td>
 <td>Image-to-Map</td>
 <td></td>
 <td>

+ DHigh performance when tuned for 100% precision
+ Allows online map feature training for real environment
 </td>
 <td>
 
+ Memory inefficient
 </td>
 </tr>
 </table>

 State-of-the-art deep learning-based loop closure detection methods for visual and Lidar SLAM:

 <table width = 100%>
 <tr>
 <th>Paper</th>
 <th>Year</th>
 <th>Sensor</th>
 <th>Components</th>
 <th>Deep Learning Algorithm</th>
 </tr>

 
 <tr>
 <td>

 Loop Closure Detection Based on Multi-Scale Deep Feature Fusion [[paper](https://www.mdpi.com/2076-3417/9/6/1120)]
 </td>
 <td>2019</td>
 <td>C</td>
 <td>CNN feature</td>
 <td>AlexNet</td>
 </tr>


  <tr>
 <td>

Loop Closure Detection for Visual SLAM Fusing Semantic Information [[paper](https://ieeexplore.ieee.org/abstract/document/8866283)]
 </td>
 <td>2019</td>
 <td>C</td>
 <td>SIFT, SURF, ORB</td>
 <td>Faster R-CNN</td>
 </tr>

  <tr>
 <td>

Improving Feature-based Visual SLAM by Semantics [[paper](https://ieeexplore.ieee.org/abstract/document/8708875)]
 </td>
 <td>2018</td>
 <td>C</td>
 <td>ORB</td>
 <td>Yolo</td>
 </tr>

  <tr>
 <td>

Lightweight Unsupervised Deep Loop Closure [[paper](https://arxiv.org/abs/1805.07703)]
 </td>
 <td>2018</td>
 <td>C</td>
 <td>HoG</td>
 <td>Autoencoder</td>
 </tr>

  <tr>
 <td>

Semantically Assisted Loop Closure in SLAM Using NDT Histograms [[paper](https://ieeexplore.ieee.org/abstract/document/8968140)]
 </td>
 <td>2019</td>
 <td>L</td>
 <td>Semantic-NDT</td>
 <td>PointNet++
 
 [[ref](https://proceedings.neurips.cc/paper_files/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html)]</td>
 </tr>

  <tr>
 <td>

LocNet: Global Localization in 3D Point Clouds for Mobile Vehicles [[paper](https://ieeexplore.ieee.org/abstract/document/8500682)]
 </td>
 <td>2018</td>
 <td>L</td>
 <td>Semi-handcrafted</td>
 <td>Siamese</td>
 </tr>

  <tr>
 <td>

SegMap: 3D Segment Mapping using Data-Driven Descriptors [[paper](https://arxiv.org/abs/1804.09557)]
 </td>
 <td>2018</td>
 <td>L</td>
 <td>SegMap</td>
 <td>CNN</td>
 </tr>

  <tr>
 <td>

SegMap: Segment-based mapping and localization using data-driven descriptors [[paper](https://journals.sagepub.com/doi/abs/10.1177/0278364919863090)]
 </td>
 <td>2018</td>
 <td>L</td>
 <td>SegMap</td>
 <td>CNN</td>
 </tr>

  <tr>
 <td>

Loop closure detection for visual SLAM using PCANet features [[paper](https://ieeexplore.ieee.org/abstract/document/7727481)]
 </td>
 <td>2016</td>
 <td>C</td>
 <td>SIFT, SURF, ORB</td>
 <td>PCANet
 
 [[ref](https://ieeexplore.ieee.org/abstract/document/7234886)]</td>
 </tr>

  <tr>
 <td>

OverlapNet: Loop Closing for LiDAR-based SLAM [[paper](https://arxiv.org/abs/2105.11344)]
 </td>
 <td>2020</td>
 <td>L</td>
 <td>Semantic class</td>
 <td>RangeNet++
 
 [[ref](https://ieeexplore.ieee.org/abstract/document/8967762)]</td>
 </tr>

  <tr>
 <td>

Compressed Holistic ConvNet Representations for Detecting Loop Closures in Dynamic Environments [[paper](https://ieeexplore.ieee.org/abstract/document/9043481)]
 </td>
 <td>2020</td>
 <td>C</td>
 <td>CNN feature</td>
 <td>ResNet18
 
 [[ref](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)]</td>
 </tr>

  <tr>
 <td>

Comparison of camera-based and 3D LiDAR-based place recognition across weather conditions [[paper](https://ieeexplore.ieee.org/abstract/document/9305429)]
 </td>
 <td>2020</td>
 <td>C/L</td>
 <td>CNN feature</td>
 <td>VGG16
 
 [[ref](https://arxiv.org/abs/1409.1556)] </td>
 </tr>

  <tr>
 <td>

Single-View Place Recognition under Seasonal Changes [[paper](https://arxiv.org/abs/1808.06516)]
 </td>
 <td>2018</td>
 <td>C</td>
 <td>CNN feature</td>
 <td>VGG16
 
 [[ref](https://arxiv.org/abs/1409.1556)]</td>
 </tr>

  <tr>
 <td>

Condition-Invariant Multi-View Place Recognition [[paper](https://arxiv.org/abs/1902.09516)]
 </td>
 <td>2019</td>
 <td>C</td>
 <td>CNN Multiview descriptor</td>
 <td>ResNet-50
 
 [[ref](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)]</td>
 </tr>

  <tr>
 <td>

Loop Closure Detection Based on Improved Hybrid Deep Learning Architecture [[paper](https://ieeexplore.ieee.org/abstract/document/8982643)]
 </td>
 <td>2019</td>
 <td>C</td>
 <td>Semantic feature</td>
 <td>Hybrid
 
 [[ref](https://arxiv.org/abs/1501.04158)]
 </td>
 </tr>

 </table>

Important resources:

+ [LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM](https://arxiv.org/abs/2103.05056)
+ [Role of Deep Learning in Loop Closure Detection for Visual and Lidar SLAM: A Survey](https://www.mdpi.com/1424-8220/21/4/1243)
+ [The Revisiting Problem in Simultaneous Localization and Mapping: A Surveyon Visual Loop Closure Detection](https://arxiv.org/ftp/arxiv/papers/2204/2204.12831.pdf)

