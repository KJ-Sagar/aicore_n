# Loop Closure

Loop closure is the process of identifying and correcting accumulated pose estimation errors when a robot revisits a previously visited location. It involves recognizing that the current sensor measurements align with a previously recorded portion of the map, thus closing a loop in the trajectory. Loop closure detection is indispensable for long-term and large-scale SLAM operations, as it helps mitigate drift and ensures a globally consistent map.

Goal :

    + Understand why loop closure is needed in SLAM.
    + Understand the principles of the bag-of-words model.
    + Use DBoW3 to detect similar images.

We know that the primary purpose of SLAM (frontend, backend) is to estimate camera movement. However, the loop closure module is quite different.

### Loop Closure and Detection :

Problem Statement: The frontend provides short-time trajectory/landmarks estimation and the map’s initial value. The backend is responsible for optimizing all these data. However, if we only consider the adjacent keyframes like VO, then the errors will inevitably accumulate with time so that the entire SLAM will suffer from the accumulative error. The result of long-term estimation will not be reliable. In other words, we cannot construct globally consistent trajectories and maps.

##  Bag of Words (BoW) :

The purpose of Bag-of-Words (BoW) is to describe an image in terms of “what
kinds of features are there on the image.” For example, we say a person and a car in
one photo; and two people and a dog in another photo. According to this description,
the similarity of the two images can be measured. To be more specific, we need to
do the following things:
1. Determine the concepts of people, cars, and dogs-corresponding to the word.
Many words are put together to form a dictionary.
2. Detect which predefined words in the dictionary appear in an image-we use the
appearance of words (histogram) to describe the entire image. In this way, we
convert an image into a vector description.
3. Compute the similarity by the histogram in the second step.

[ Train the dictionary, Calculate the similarity ]

```cpp

#include "DBoW3/DBoW3.h"
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <iostream>
#include <vector>
#include <string>

using namespace cv;
using namespace std;

/**************************************************** **
  * This section demonstrates how to calculate the similarity score based on the previously trained dictionary
  * ***************************************************/
int main(int argc, char **argv) {
     // read the images and database
     cout << "reading database" << endl;
     DBoW3::Vocabulary vocab("./vocabulary.yml.gz");
     // DBoW3::Vocabulary vocab("./vocab_larger.yml.gz"); // use large vocab if you want:
     if (vocab. empty()) {
         cerr << "Vocabulary does not exist." << endl;
         return 1;
     }
     cout << "reading images... " << endl;
     vector<Mat> images;
     for (int i = 0; i < 10; i++) {
         string path = "./data/" + to_string(i + 1) + ".png";
         images.push_back(imread(path));
     }

     // NOTE: in this case we are comparing images with a vocabulary generated by themselves,
     //  this may lead to overfit.
     // detect ORB features
     cout << "detecting ORB features ... " << endl;
     Ptr<Feature2D> detector = ORB::create();
     vector<Mat> descriptors;
     for (Mat &image:images) {
         vector<KeyPoint> keypoints;
         Mat descriptor;
         detector->detectAndCompute(image, Mat(), keypoints, descriptor);
         descriptors.push_back(descriptor);
     }

     // we can compare the images directly or we can compare one image to a database
     // images:
     cout << "comparing images with images " << endl;
     for (int i = 0; i < images. size(); i++) {
         DBoW3::BowVector v1;
         vocab. transform(descriptors[i], v1);
         for (int j = i; j < images. size(); j++) {
             DBoW3::BowVector v2;
             vocab. transform(descriptors[j], v2);
             double score = vocab. score(v1, v2);
             cout << "image " << i << " vs image " << j << " : " << score << endl;
         }
         cout << endl;
     }

     // or with database
     cout << "comparing images with database " << endl;
     DBoW3::Database db(vocab, false, 0);
     for (int i = 0; i < descriptors. size(); i++)
         db.add(descriptors[i]);
     cout << "database info: " << db << endl;
     for (int i = 0; i < descriptors. size(); i++) {
         DBoW3::QueryResults ret;
         db. query(descriptors[i], ret, 4); // max result=4
         cout << "searching for image " << i << " returns " << ret << endl << endl;
     }
     cout << "done." << endl;
}

```

[ Increasing the Dictionary Scale, Similarity Score Processing, Processing the Keyframes, Validation of the Detected Loops, ML in Loop Closing ]

Important resources:

+ [LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM](https://arxiv.org/abs/2103.05056)
+ [The Revisiting Problem in Simultaneous Localization and Mapping: A Surveyon Visual Loop Closure Detection](https://arxiv.org/ftp/arxiv/papers/2204/2204.12831.pdf)

